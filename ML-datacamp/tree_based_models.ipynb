{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "### Doubts\n",
    "1. Decision tree produces rectangular decision regions in a feature space, this happens because at each split made by the tree, only 1 feature is involved. `What does this mean ?` \n",
    "    - Ans: Try to visualize this you will understand, if we 2 or more features to make a split, feature space after the split will not be rectangular. \n",
    "    - But if we take 1 feature then the shape of feature space will always be rectangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree\n",
    "### Decision Tree\n",
    "1. Provided labelled data, Learns a sequence of if-else questions about individual features \n",
    "2. Two type of Node\n",
    "    - Question Node\n",
    "        - Root and non-leaf Node\n",
    "            - if-else question { feature(f), split point(sp)}\n",
    "    - Prediction Node\n",
    "        - Predicted Label: prevailing label among data points in the end node.\n",
    "        - May/May not be leaf node\n",
    "3. Learning\n",
    "    - The tree learns pattern from features in such a way to produce the purest leafs \n",
    "        - Purest Leafs: In each leaf, one class label is predominant \n",
    "4. Impurity of a Node ??\n",
    "    - How to measure the impurity \n",
    "        - Criterion \n",
    "            - gini index \n",
    "            - entropy \n",
    "5. Information Gain ??\n",
    "    - Formula: see pdf\n",
    "    - Informatin Gain (IG)\n",
    "        - IG(node) : Information gain obtained by splitting the node\n",
    "        - Leaf Node: IG(node)=0\n",
    "        \n",
    "6. How it produces the purest leaf possible ?\n",
    "    - At each node it asks a question involving one feacture (f) and split point (sp)\n",
    "        - But how it decides which feature and split point to Pick ?\n",
    "        - By maximising the information gain \n",
    "        - The tree considers that every node contains some information and it aims to maximize the information gain obtained after each split.\n",
    "            - So whichever combo of (f,sp) gives max IG is selected.\n",
    "7. Unconstrained Tree \n",
    "    - Means no max_depth defined, prediction made by leaf nodes \n",
    "    - Leaf Node = Prediction Nodes\n",
    "8. Constrained Tree\n",
    "    - Max depth defined \n",
    "    - If max depth =d , then all the nodes at depth 'd' become the prediction nodes, doesn't matter if their IG(node) = 0 or != 0.\n",
    "    \n",
    "### Thoughts \n",
    "- So at each node we have to decide which feature and split point to use. It is analogus to selecting parameters for a model. \n",
    "    - We select parameters which minimizes the loss. \n",
    "    - So to select out of multiple options we need some criteria just like we use loss or accuracy.\n",
    "    - To select the best feature and split point, criteria is information gain. \n",
    "        - Whichever combination maximises the IG , will be selected \n",
    "- At each node we have some information with us, in the form of data points, data points are information\n",
    "    - Now when we made a split, we logically splitted our data points, now we have our data points in two different categories, data points in each category have some common properties that's why they landed in that category. So we can say we have gained additional information by making that split. So after a split we have Information Gain. This Gain we want to maximize at each point. \n",
    "- This algorithm looks greedy to me, as at each node we select feature and split point which maximizes the information gain.\n",
    " \n",
    "## Regression Tree \n",
    "### Decision Tree\n",
    "1. Target variable is continous, output of model is real value.\n",
    "2. min_samples_leaf \n",
    "    - min_samples leaf = 0.1 \n",
    "        - Stopping condition, each leaf must contains atlest 10% of training data \n",
    "        - No of data points >= 10%\n",
    "3. Impurity of node \n",
    "    - *I(node)* = mean squared error(MSE) of targets in that node \n",
    "        - `I(node) = (1/N)*summation( y(i) - ymean )**2` ; N = no of data points in the node   \n",
    "        - `ymean = (1/N)*summation(y(i))`\n",
    "    - This means regression tree tries to find the split that produces leaf nodes wherein each leaf node, the target value on average is closest to the mean target variable of that node.\n",
    "        - This implies it tries to minimize the Impurity of leaf node. \n",
    "4. Prediction \n",
    "    - ypredict = average of the target variables in the prediction node \n",
    "    - `ypredict = (1/N)*summation(y(i))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
