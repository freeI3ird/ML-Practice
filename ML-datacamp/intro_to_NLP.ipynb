{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex\n",
    "`Link: https://www3.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html`\n",
    "1. Special Character Classes in Regex pattern\n",
    "    - \\s : matches space \n",
    "    - \\S : anything that is not space \n",
    "    - Using Capital Characters negates the class \n",
    "2. Python Module `import re`\n",
    "    - In its function, first arguement is PATTERN then STRING\n",
    "    - Important methods \n",
    "        - `re.search(pattern, string)`\n",
    "        - `re.match(pattern, string)`\n",
    "        - `re.findall(pattern, string)`\n",
    "        - `re.sub(pattern, new_substring, string)`\n",
    "        - `re.split(pattern, string)`\n",
    "3. Character Range \n",
    "    - To define a new character class \n",
    "    - E.g: character class with small and capital alphabets, space, hyphen, dot\n",
    "        - `regex: r\"[a-zA-Z\\-\\. ]\"`\n",
    "        - hyphen and dot are speciall characters need to escape them.\n",
    "4. Group \n",
    "    - To define explicit patterns\n",
    "    - E.g \n",
    "        - r\"(a-z)\" : This will match with exact string as \"a-z\", hyphen don't have special meaning here as in range.\n",
    "5. or method : | (pipe)\n",
    "    - E.g \n",
    "        - r\"(\\s+|,)\": This will match spaces( ) or comman(,) \n",
    "\n",
    "6. How to write unicode in the regex\n",
    "\n",
    "7. How to use new line character in regex '\\n', can we use it in raw string r\"\\n\"\n",
    "8. Learn more about raw string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "`Read about it in pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', '!', 'I', 'am', 'Robo', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Hello World! I am Robo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words \n",
    "`Refer pdf`\n",
    "1. More frequent a word is, more relevant/centered to the text it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 2,\n",
       "         'loves': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(word_tokenize(\"The cat is in the box. The cat loves the box. The box is over the cat\"))\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3), ('the', 3), ('box', 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "`Refer pdf`\n",
    "1. Lowercase\n",
    "2. Tokenization\n",
    "2. Stopwords Removal \n",
    "3. Stemming/Lemmatization\n",
    "4. Remove Punctuation, stopwords and unwanted tokens\n",
    "\n",
    "`Methods`\n",
    "1. \"sample_string\".isalpha()\n",
    "    - Checks if string contains alphabets only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The cat is in the box. The cat loves the boxes. The box is over the cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_only = [w for w in word_tokenize(lower_text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'in', 'the', 'box', 'the', 'cat', 'loves', 'the', 'boxes', 'the', 'box', 'is', 'over', 'the', 'cat']\n"
     ]
    }
   ],
   "source": [
    "print(alpha_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'box', 'cat', 'loves', 'boxes', 'box', 'cat']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stops = [ w for w in alpha_only if w not in stopwords.words('english')]\n",
    "no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'box', 'cat', 'love', 'box', 'box', 'cat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [ WordNetLemmatizer().lemmatize(w) for w in no_stops]\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'cat', 'is', 'in', 'the', 'box'],\n",
       " ['The', 'cat', 'loves', 'the', 'boxes'],\n",
       " ['The', 'box', 'is', 'over', 'the', 'cat']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ word_tokenize(w) for w in text.split('.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim \n",
    "1. Open Source NLP library, uses top models to perfrom complex tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of documents\n",
    "mydocs = [\"The cat is in the box.\",\"The cat loves the boxes.\",\"The box is over the cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cat', 'box'], ['cat', 'loves', 'boxes'], ['box', 'cat']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase, tokenise ,stopwords removed, punctuation removed\n",
    "articles=[[ word for word in word_tokenize(doc.lower()) if word not in stopwords.words('english') if word.isalpha() \\\n",
    "          ] for doc in mydocs]\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The cat is in the box.',\n",
       " 'The cat loves the boxes.',\n",
       " 'The box is over the cat']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gDict = Dictionary(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'box': 0, 'cat': 1, 'boxes': 2, 'loves': 3}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gDict.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gDict.doc2bow([\"cat\", \"loves\", \"other\", \"cat\", \"babies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1)], [(1, 1), (2, 1), (3, 1)], [(0, 1), (1, 1)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating corpus \n",
    "mycorpus = [ gDict.doc2bow(article) for article in articles]\n",
    "mycorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF \n",
    "`Refer the pdf`\n",
    "1. Code\n",
    "    - `from gensim.models.tfidfmodel import TfidfModel`\n",
    "    - `tfidf = TfidfModel(corpus)`\n",
    "        - `corpus = [[(id1, freq1), (id2, freq2), (id3, freq3)], [ (id4 freq4, (id5 freq5)]`\n",
    "    - `tfidf[doc]` : gives tfidf weights for sample doc \n",
    "        - `[ (id1, weight1), (id2, weight2), (id3, weight3) ]`\n",
    "2. TF: term frequency\n",
    "    - tf(i) = No of occurences of word(i) in the document(j)\n",
    "        - If the documents have different length then \n",
    "        - tf = ( #occurences of word)/(total no of words in doc)\n",
    "3. Idf: Inverse Document Frequency\n",
    "    - idf(i,j) = log(N/df)\n",
    "        - N = Total no of documents\n",
    "        - df = No of documents in which word/token(i) is present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER: Named Entity Recognition \n",
    "1. Identifying important entities in the text. \n",
    "2. Code:\n",
    "    - `tokenize_sent = word_tokenize(sentence)`\n",
    "    - `tagged_sent = nltk.pos_tag(tokenize_sent)`\n",
    "    - `nltk.ne_chunk(tagged_sent)`\n",
    "        - Chunk the Tagged sentence into name entity chunks\n",
    "        - For this task it uses trained statistical and grammatical parsers not some knowledge base like wikipedia\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "1. Open source Nlp library similar to gensim but different implementation \n",
    "2. Another option for NLP tasks \n",
    "\n",
    "### Displacy \n",
    "1. Visualization tool built by makers of Spacy. \n",
    "2. Used to visualize the Parse Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyglot \n",
    "1. Open Source NLP library similar to gensim and Spacy \n",
    "2. It supports operations and have word vectors for a large no of languages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer and TfidfVectorizer \n",
    "1. Bag of words(bow) and Tfidf classes of scikit-learn library \n",
    "2. Methods \n",
    "    - `.fit()`\n",
    "        - This method in general tries to find the parameters or norms of data\n",
    "    - `.transform()`\n",
    "        - It applies the underlying algorithm or approximation on data\n",
    "    - `.fit_transform()`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes \n",
    "1. `from sklearn.naive_bayes import MultinomialNB`\n",
    "2. Naive Bayes works well on NLP tasks \n",
    "    - It basis on probability. \n",
    "    - It ans questions like: Given a particular piece of data how likely is a particular outcome\n",
    "3. It works on probability. \n",
    "4. MultinomialNB\n",
    "    - Works well when features have integer values \n",
    "    - Works well for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix \n",
    "1. By default, at the top side we have predict labels and left side actual labels\n",
    "2. `metrics.confusion_matrix(ytest, ypredict, labels=[0,1])`\n",
    "    - Specifying the labels removes confusion, 1st column of confusion matrix will have label=0 and other column=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
